{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gastric Cancer Detection using resnet_50 and histopathology image patches"
      ],
      "metadata": {
        "id": "8vRkpPJfLEIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   This notebook entails the creation of a gastric cancer detection model that uses image patches and the resnet_50 pretrained CNN model.\n",
        "*   The dataset used can be accessed from this site :  https://figshare.com/articles/dataset/GasHisSDB/15066147/1\n",
        "*   The dataset was divided separately as follows : 70% training, 15% validation, and 15% testing.\n",
        "*   This code was run on Google Colab using T4 GPU.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0p6bAQhOCsDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import the necessary libraries."
      ],
      "metadata": {
        "id": "IJ9uW1cWLsx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfNnTUzvf4hm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Set the device."
      ],
      "metadata": {
        "id": "ClqwPOheL9MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9HYxyt5qho5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Connect to Google Drive to access the zip folders containing the image patches."
      ],
      "metadata": {
        "id": "uglR9Gp2MIfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YAzE5sqiiwc",
        "outputId": "ab4d0571-2af3-4425-d5a9-7e5bb891797a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH=\"/content/gdrive/MyDrive/GasHisDB/the_resnet_50_model.pth\" #path where the trained model will be saved"
      ],
      "metadata": {
        "id": "PJPWMgPZmCSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define the image transformation pipelines."
      ],
      "metadata": {
        "id": "d9qkHVZQM1ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "\n",
        "    'Train' : transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Validate': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "8uHS4Nf5mhk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Extract the images from the zips."
      ],
      "metadata": {
        "id": "zCttdZ_OM4Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don't run again\n",
        "#extract dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "#Abnormal\n",
        "ZIP_PATH_120 = \"/content/gdrive/MyDrive/GasHisDB/120_dataset.zip\"\n",
        "IMAGE_FOLDER_120 = \"/content/gdrive/MyDrive/GasHisDB/120_b\"\n",
        "\n",
        "# Ensure that the extraction folder exists\n",
        "if not os.path.exists(IMAGE_FOLDER_120):\n",
        "    os.makedirs(IMAGE_FOLDER_120)\n",
        "\n",
        "# Open the zip file and extract its contents without creating a new folder\n",
        "with zipfile.ZipFile(ZIP_PATH_120, 'r') as zip_ref:\n",
        "    for file_info in zip_ref.infolist():\n",
        "        print(f\"Extracting: {file_info.filename}\")\n",
        "        zip_ref.extract(file_info, path=IMAGE_FOLDER_120)"
      ],
      "metadata": {
        "id": "n8XChq85o1MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/GasHisDB/120_b/120_dataset/Train/Normal/Normal-02185.png"
      ],
      "metadata": {
        "id": "7_4IzD-G-Pul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create data loaders for training and validation image datasets."
      ],
      "metadata": {
        "id": "HFeBPAxFM92Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/GasHisDB/120_b/120_dataset'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['Train', 'Validate']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2) for x in ['Train', 'Validate']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['Train', 'Validate']}\n",
        "class_names = image_datasets['Train'].classes"
      ],
      "metadata": {
        "id": "FOmXMwBhmnZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Define the function for training the model."
      ],
      "metadata": {
        "id": "HRgIQ0iGNXQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training the model\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['Train', 'Validate']:\n",
        "            if phase == 'Train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'Train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'Train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'Validate' and epoch_acc > best_acc:\n",
        "\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), './best-model-checkpoint.pt')\n",
        "                best_acc = epoch_acc\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-ZL8E4x6pRZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Load the resnet_50 pretrained model and train the model while performing validation."
      ],
      "metadata": {
        "id": "xqqGnYWHNecs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ],
      "metadata": {
        "id": "h8Kf6AA7pdmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSaving the model...\")\n",
        "torch.save(model_ft, PATH)\n",
        "print(\"\\nThe Model is Saved...Hurray\")"
      ],
      "metadata": {
        "id": "sEheV1xNpv86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/GasHisDB/120/120_dataset/Test/Normal/Normal-01006.png"
      ],
      "metadata": {
        "id": "JZUTmlgrHIaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Test an image using the model"
      ],
      "metadata": {
        "id": "sc88YIJdNola"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test a single image\n",
        "from PIL import Image\n",
        "\n",
        "# Load the saved model checkpoint\n",
        "model = torch.load(PATH)\n",
        "model.eval()\n",
        "\n",
        "# Define the transformations for the test image\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and preprocess the test image\n",
        "test_image_path = '/content/gdrive/MyDrive/GasHisDB/120/120_dataset/Test/Normal/Normal-01006.png'  # the path to test image\n",
        "test_image = Image.open(test_image_path)\n",
        "input_image = data_transforms(test_image).unsqueeze(0)  # Add a batch dimension\n",
        "#'input_image' is input tensor and 'device' is device (cuda)\n",
        "input_image = input_image.to(device)\n",
        "\n",
        "# Perform inference on the test image\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_image)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Softmax to get probabilities\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Get the predicted class index and class name\n",
        "predicted_class_index = predicted.item()\n",
        "class_names = ['abnormal', 'normal']\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "predicted_probability = probabilities[0][predicted_class_index].item() * 100  # Probability in percentage\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class_name}\")\n",
        "print(f\"The probability of being {predicted_class_name} is: {predicted_probability:.2f}%\")\n"
      ],
      "metadata": {
        "id": "X_4h_RjKsqq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(PATH)"
      ],
      "metadata": {
        "id": "Gy_LfKcvcB_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Test the model using the test dataset."
      ],
      "metadata": {
        "id": "6SCSPZ6MORco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FinalTestfolder\n",
        "ZIP_PATH_120 = \"/content/gdrive/MyDrive/GasHisDB/GasHisDB_Final_Test.zip\"\n",
        "IMAGE_FOLDER_120 = \"/content/gdrive/MyDrive/GasHisDB/GasHisDB_Final_Test\"\n",
        "\n",
        "# Ensure that the extraction folder exists\n",
        "if not os.path.exists(IMAGE_FOLDER_120):\n",
        "    os.makedirs(IMAGE_FOLDER_120)\n",
        "\n",
        "# Open the zip file and extract its contents without creating a new folder\n",
        "with zipfile.ZipFile(ZIP_PATH_120, 'r') as zip_ref:\n",
        "    for file_info in zip_ref.infolist():\n",
        "        print(f\"Extracting: {file_info.filename}\")\n",
        "        zip_ref.extract(file_info, path=IMAGE_FOLDER_120)"
      ],
      "metadata": {
        "id": "b1f1iF8idFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transformation for test data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to the input size of the model\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with the same values used in training\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.ImageFolder(root='/content/gdrive/MyDrive/GasHisDB/GasHisDB_Final_Test', transform=test_transform)\n",
        "\n",
        "# Create the DataLoader for test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "Kg2QPMcQc-28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(PATH)\n",
        "model = model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "probs = []\n",
        "\n",
        "with torch.no_grad(): #no need to calculate gradients, only necessary during training when we want to update the weights\n",
        "    for inputs, labels in test_loader: #yields batches of input data and corresponding labels from the test dataset\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1) #softmax function is applied across the second dimension (dim=1), which corresponds to the class probabilities for each input.\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)# finds the maximum value in the outputs along dimension 1, which represents the most likely class according to the model, only returns predicted class nt max value\n",
        "        true_labels.extend(labels.cpu().numpy()) # true labels for the batch are converted from a PyTorch tensor to a NumPy array and added to the true_labels list\n",
        "        predictions.extend(predicted.cpu().numpy()) # the predicted classes for the batch are converted to a NumPy array and added to the predictions list\n",
        "        probs.extend(probabilities.cpu().numpy())"
      ],
      "metadata": {
        "id": "aSj1FOPzRKQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "print(classification_report(true_labels, predictions))\n",
        "print(f\"Accuracy: {accuracy_score(true_labels, predictions)}\")\n",
        "\n",
        "# Calculate the ROC-AUC score and plot the ROC curve\n",
        "# Assuming your positive class is the second one in the 'class_names'\n",
        "positive_class_probs = np.array(probs)[:, 1]\n",
        "roc_auc = roc_auc_score(true_labels, positive_class_probs)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(true_labels, positive_class_probs)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WPmTlBeVhgxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Visualize the test images along with the predictions."
      ],
      "metadata": {
        "id": "xcGtXVJYOhUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
      ],
      "metadata": {
        "id": "PNf5MCf2itUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'true: {class_names[labels[j]]} pred: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    return\n"
      ],
      "metadata": {
        "id": "AiivbHYtiu_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model_ft, num_images=6)"
      ],
      "metadata": {
        "id": "2TeP088ciyS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Generate heatmaps to highlight regions of high influence to the predictions being made."
      ],
      "metadata": {
        "id": "tc7lHEmLOpVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#load model\n",
        "model_ft = torch.load(PATH)\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.eval()\n",
        "\n",
        "# Define preprocessing transformations\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load and preprocess your medical image\n",
        "image_path = '/content/gdrive/MyDrive/GasHisDB/GasHisDB_Final_Test/Abnormal/Abnormal-04038.png'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = preprocess(img)\n",
        "input_batch = input_tensor.unsqueeze(0).to(device)  # Add a batch dimension and move to GPU\n",
        "\n",
        "# Set up hooks for guided backpropagation\n",
        "activation = []\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activation.append(output)\n",
        "\n",
        "for module in model_ft.named_modules():\n",
        "    if isinstance(module[1], torch.nn.ReLU):\n",
        "        module[1].register_forward_hook(hook_fn)\n",
        "\n",
        "# Forward pass\n",
        "output = model_ft(input_batch)\n",
        "prediction = output.argmax(1)\n",
        "\n",
        "# Backpropagation to compute gradients\n",
        "model_ft.zero_grad()\n",
        "one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(device)\n",
        "one_hot_output[0][prediction] = 1\n",
        "output.backward(gradient=one_hot_output)\n",
        "\n",
        "# Get the gradients and compute the heatmap\n",
        "if len(activation) > 0:\n",
        "    gradients = activation[0][0].detach().cpu().numpy()\n",
        "    heatmap = gradients.max(axis=0)\n",
        "\n",
        "    # Apply thresholding to the heatmap\n",
        "    threshold = 0.5  # Adjust this threshold value as needed\n",
        "    heatmap[heatmap < threshold] = 0\n",
        "\n",
        "    # Visualize the heatmap\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.imshow(heatmap, cmap='hot')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No activations recorded.\")\n"
      ],
      "metadata": {
        "id": "rcQYe-gK3AS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Test the model with a sample image patch."
      ],
      "metadata": {
        "id": "WuQBmp_ADrvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#load model\n",
        "model_ft = torch.load(PATH)\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.eval()\n",
        "\n",
        "# Define preprocessing transformations for prediction\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and preprocess the image for prediction\n",
        "image_path = '/content/gdrive/MyDrive/GasHisDB/GasHisDB_Final_Test/Abnormal/Abnormal-04038.png'\n",
        "test_image = Image.open(image_path)\n",
        "input_image = data_transforms(test_image).unsqueeze(0).to(device)  # Add a batch dimension\n",
        "\n",
        "# Perform inference on the test image\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_image)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Softmax to get probabilities\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Get the predicted class index and class name\n",
        "predicted_class_index = predicted.item()\n",
        "class_names = ['abnormal', 'normal']  # Replace with your class names\n",
        "\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "predicted_probability = probabilities[0][predicted_class_index].item() * 100  # Probability in percentage\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class_name}\")\n",
        "print(f\"The probability of being {predicted_class_name} is: {predicted_probability:.2f}%\")\n",
        "\n",
        "# Set up hooks for guided backpropagation\n",
        "activation = []\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activation.append(output)\n",
        "\n",
        "for module in model_ft.named_modules():\n",
        "    if isinstance(module[1], torch.nn.ReLU):\n",
        "        module[1].register_forward_hook(hook_fn)\n",
        "\n",
        "# Forward pass\n",
        "output = model_ft(input_batch)\n",
        "prediction = output.argmax(1)\n",
        "\n",
        "# Backpropagation to compute gradients\n",
        "model_ft.zero_grad()\n",
        "one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(device)\n",
        "one_hot_output[0][prediction] = 1\n",
        "output.backward(gradient=one_hot_output)\n",
        "\n",
        "# Get the gradients and compute the heatmap\n",
        "if len(activation) > 0:\n",
        "    gradients = activation[0][0].detach().cpu().numpy()\n",
        "    heatmap = gradients.max(axis=0)\n",
        "\n",
        "    # Apply thresholding to the heatmap\n",
        "    threshold = 0.5  # Adjust this threshold value as needed\n",
        "    heatmap[heatmap < threshold] = 0\n",
        "\n",
        "    # Visualize the heatmap\n",
        "    # Descriptions of Red and Yellow regions in a heatmap\n",
        "    print(\"Red Regions -  highest importance for the predicted class.\")\n",
        "    print(\"Yellow Regions - moderately high importance for the predicted class.\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.imshow(heatmap, cmap='hot')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No activations recorded.\")"
      ],
      "metadata": {
        "id": "-CgKi-Nx55JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Test the model using a whole slide image from which the 120 by 120 patches were extracted."
      ],
      "metadata": {
        "id": "ilfwswmRDkm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#load model\n",
        "model_ft = torch.load(PATH)\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.eval()\n",
        "\n",
        "# Define preprocessing transformations for prediction\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and preprocess the image for prediction\n",
        "image_path = '/content/gdrive/MyDrive/HCRF/test_dataset/cancer_108.png'\n",
        "test_image = Image.open(image_path)\n",
        "input_image = data_transforms(test_image).unsqueeze(0).to(device)  # Add a batch dimension\n",
        "\n",
        "# Perform inference on the test image\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_image)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Softmax to get probabilities\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Get the predicted class index and class name\n",
        "predicted_class_index = predicted.item()\n",
        "class_names = ['abnormal', 'normal']  #  class names\n",
        "\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "predicted_probability = probabilities[0][predicted_class_index].item() * 100  # Probability in percentage\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class_name}\")\n",
        "print(f\"The probability of being {predicted_class_name} is: {predicted_probability:.2f}%\")\n",
        "\n",
        "# Set up hooks for guided backpropagation\n",
        "activation = []\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activation.append(output)\n",
        "\n",
        "for module in model_ft.named_modules():\n",
        "    if isinstance(module[1], torch.nn.ReLU):\n",
        "        module[1].register_forward_hook(hook_fn)\n",
        "\n",
        "# Forward pass\n",
        "output = model_ft(input_image)\n",
        "prediction = output.argmax(1)\n",
        "\n",
        "# Backpropagation to compute gradients\n",
        "model_ft.zero_grad()#to save memory\n",
        "one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(device)\n",
        "one_hot_output[0][prediction] = 1\n",
        "output.backward(gradient=one_hot_output)\n",
        "\n",
        "# Get the gradients and compute the heatmap\n",
        "if len(activation) > 0:\n",
        "    gradients = activation[0][0].detach().cpu().numpy()\n",
        "    heatmap = gradients.max(axis=0)\n",
        "\n",
        "    # Apply thresholding to the heatmap\n",
        "    threshold = 0.5  # Adjust this threshold value as needed\n",
        "    heatmap[heatmap < threshold] = 0\n",
        "\n",
        "    # Visualize the heatmap\n",
        "    # Descriptions of Red and Yellow regions in a heatmap\n",
        "    print(\"Red Regions -  highest importance for the predicted class.\")\n",
        "    print(\"Yellow Regions - moderately high importance for the predicted class.\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.imshow(heatmap, cmap='hot')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No activations recorded.\")"
      ],
      "metadata": {
        "id": "WM7yoHKV9TSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}